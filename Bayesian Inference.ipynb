{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference\n",
    "\n",
    "Often we have some belief and are presented with some observation that agrees more or less with that belief. How are we to incorporate this new observation into our beliefs? We could formulate hypotheses and test for statistical significance, but there are some serious shortcomings to such a technique. First, how do we choose an alternative hypothesis. Second, p-values tell us the probability of making an observation given a belief being true, but usually we want to know the probability of a belief being true given an observation. These problems can be addressed via Bayes' theorem, which allows us to quantitatively update the credibility of a belief in response to new data.\n",
    "\n",
    "$$P(X | e) = \\frac{P(e | X) \\cdot P(X)}{P(e)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine there is a virus going around, and roughly 20% of your coworkers have it. You figure you have a 20% chance of you having it too, and you decide to get tested. The doctor swabs the back of your throat and tells you that you test positive. She also tells you that 90% of sick people test positively, as well as 30% of healthy people. What does that mean for you? You sit down and make a chart and pull up Bayes' theorem.\n",
    "\n",
    "||Sick (20%)|Healthy(80%)|\n",
    "|-------------|---|---|\n",
    "|Positive Test|18%|24%|\n",
    "|Negative Test|2%|56%|\n",
    "\n",
    "Here we're interested in the probability that you are sick ($X=$ 'sick') given the positive test ($e=$ 'positive'). We know the probability of a positive test given that you are sick is 90%, and your prior belief (i.e. $P(x)$) about the possibility that you're sick is a probability of 20%. Furthermore, we can see that the marginal probability of a positive test is 42%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New probability of being sick: 0.428571428571\n"
     ]
    }
   ],
   "source": [
    "print('New probability of being sick: ' + str(.9 * .2 / .42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out there's only a 42% chance that you're sick! Why didn't the test make us more certain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood ratios\n",
    "\n",
    "The ratio probability of making an observation given a belief being true to the probability of of making that observation under any circumstances is called the likelhood ratio. The likelihood ratio is the factor that modifies our prior belief, so to have a discerning test, we want a large likelihood ratio. Since the numerator and denominator are both probabilities, they are bound between 0 and 1, which means that to make a large likelihood ratio, the key is to choose a test that is very unlikely to happen in the absence of $X$. In the case of our doctor's swab, the 30% false positive undermines the test.\n",
    "\n",
    "![Bayes' illustration](waterfall.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihood ratios also make Bayes' theorem a bit easier to think about. Initially you believed there was 1:4 odds you were sick. The doctor's test had a 3:1 likelihood ratio (90% positive for sick people, 30% positive for healthy people). Therefore you now believe there's 3:4 odds (i.e. 43% chance) that you're sick.\n",
    "\n",
    "### Designing discerning tests\n",
    "\n",
    "Having a high likelihood ratio indicates we've designed a strong test, making it unlikely for us to mistakenly believe you're healthy when you're really sick. This sounds similar to Type 2 error in our discussion of hypothesis testing, in which we had to adopt a sufficiently distinct alternative hypothesis to avoid the error of failing to reject the null hypothesis even when it was wrong. Here we have the advantage that we don't have to specify our posterior belief in advance, and instead our power derives from the design of our tests.\n",
    "\n",
    "We can also think about the test design in terms of precision and recall. Due to the 90% positive rate among sick people, our test has high recall (i.e. almost everyone who's sick will test positive), but it has low precision (i.e. many people who are not sick will also test positive). Depending on how rare or common a sickness it, it may become more or less beneficial to favor precision or recall, and we can adjust our tests or thresholds accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tdi-venv)",
   "language": "python",
   "name": "tdi-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
